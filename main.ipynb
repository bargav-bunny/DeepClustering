{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('Data/dblp.txt')\n",
    "y = np.loadtxt('Data/dblp_label.txt', dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4057, 334)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 0 ... 3 3 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 50\n",
    "\n",
    "pca = PCA(n_components=n_input)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X = torch.Tensor(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 1833],\n",
       "       [   2,   97],\n",
       "       [   2, 1561],\n",
       "       ...,\n",
       "       [4055, 1286],\n",
       "       [4055, 2202],\n",
       "       [4055, 3007]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = np.loadtxt('Data/dblp_graph.txt', dtype=np.int32)\n",
    "\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adj = sparse.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(n, n), dtype=np.float32)\n",
    "\n",
    "adj = adj + sparse.eye(adj.shape[0])\n",
    "\n",
    "degrees = np.array(adj.sum(axis=1)).flatten()\n",
    "\n",
    "D_inv_sqrt = sparse.diags(np.power(degrees, -0.5))\n",
    "\n",
    "adj = D_inv_sqrt.dot(adj).dot(D_inv_sqrt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_torch_sparse_tensor(sparse_mx):\n",
    "\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "\n",
    "    values = torch.tensor(sparse_mx.data, dtype=torch.float32)\n",
    "\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = to_torch_sparse_tensor(adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load available Device \n",
    "\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mps_device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = mps_device\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    1,    1,  ..., 4055, 4055, 4056],\n",
       "                       [   0,    1, 1833,  ..., 3007, 4055, 4056]]),\n",
       "       values=tensor([1.0000, 0.5000, 0.4082,  ..., 0.2582, 0.2000, 1.0000]),\n",
       "       size=(4057, 4057), nnz=11113, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj  = adj.to(device)\n",
    "X = X.to(device)\n",
    "\n",
    "label = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, hidden1_dim, hidden2_dim, hidden3_dim):\n",
    "        super(AE_Encoder, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Linear(in_channels, hidden1_dim)\n",
    "        self.enc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.enc3 = nn.Linear(hidden2_dim, hidden3_dim)\n",
    "        self.z_layer = nn.Linear(hidden3_dim, out_channels)\n",
    "        self.act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.act_fn(self.enc1(x))\n",
    "        z = self.act_fn(self.enc2(z))\n",
    "        z = self.act_fn(self.enc3(z))\n",
    "\n",
    "        z = self.z_layer(z)\n",
    "        \n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AE_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, hidden1_dim, hidden2_dim, hidden3_dim):\n",
    "        super(AE_Decoder, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.Linear(in_channels, hidden1_dim)\n",
    "        self.dec2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.dec3 = nn.Linear(hidden2_dim, hidden3_dim)\n",
    "        self.xhat_layer = nn.Linear(hidden3_dim, out_channels)\n",
    "        self.act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        xhat = self.act_fn(self.dec1(z))\n",
    "        xhat = self.act_fn(self.dec2(xhat))\n",
    "        xhat = self.act_fn(self.dec3(xhat))\n",
    "\n",
    "        xhat = self.xhat_layer(xhat)\n",
    "        \n",
    "        return xhat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3, n_ae_dec1, n_ae_dec2, n_ae_dec3):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = AE_Encoder(n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3)\n",
    "        self.decoder = AE_Decoder(n_z, n_input, n_ae_dec1, n_ae_dec2, n_ae_dec3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        xhat = self.decoder(z)\n",
    "\n",
    "        return xhat, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, active = False):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.active = active\n",
    "        self.fc = nn.Linear(in_channels, out_channels, )\n",
    "        self.act_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        if self.active:\n",
    "            support = self.act_fn(self.fc(x))\n",
    "        else:\n",
    "            support = self.fc(x)\n",
    "\n",
    "        return torch.spmm(adj, support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGAE_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, hidden1_dim, hidden2_dim):\n",
    "        super(IGAE_Encoder, self).__init__()\n",
    "\n",
    "        self.enc1 = GNNLayer(in_channels, hidden1_dim, active=True)\n",
    "        self.enc2 = GNNLayer(hidden1_dim, hidden2_dim, active=True)\n",
    "        self.enc3 = GNNLayer(hidden2_dim, out_channels, active=False)\n",
    "\n",
    "        self.sigmoid_fn = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        z = self.enc1(x, adj)\n",
    "        z = self.enc2(z, adj)\n",
    "        z = self.enc3(z, adj)\n",
    "\n",
    "        adj_hat = self.sigmoid_fn(torch.mm(z, z.t()))\n",
    "\n",
    "        return z, adj_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGAE_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, hidden1_dim, hidden2_dim):\n",
    "        super(IGAE_Decoder, self).__init__()\n",
    "\n",
    "        self.dec1 = GNNLayer(in_channels, hidden1_dim, active = True)\n",
    "        self.dec2 = GNNLayer(hidden1_dim, hidden2_dim, active = True)\n",
    "        self.dec3 = GNNLayer(hidden2_dim, out_channels, active=True)\n",
    "\n",
    "        self.sigmoid_fn = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z, adj):\n",
    "        xhat = self.dec1(z, adj)\n",
    "        xhat = self.dec2(xhat, adj)\n",
    "        xhat = self.dec3(xhat, adj)\n",
    "\n",
    "        adj_hat = self.sigmoid_fn(torch.mm(xhat, xhat.t()))\n",
    "\n",
    "        return xhat, adj_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGAE(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_z, n_igae_enc1, n_igae_enc2, n_igae_dec1, n_igae_dec2):\n",
    "        super(IGAE, self).__init__()\n",
    "\n",
    "        self.encoder = IGAE_Encoder(n_input, n_z, n_igae_enc1, n_igae_enc2)\n",
    "        self.decoder = IGAE_Decoder(n_z, n_input, n_igae_dec1, n_igae_dec2)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        z, adj_enc = self.encoder(x, adj)\n",
    "        xhat, adj_dec = self.decoder(z, adj)\n",
    "\n",
    "        adj_hat = adj_enc + adj_dec\n",
    "\n",
    "        return z, xhat, adj_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_node, n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3,n_ae_dec1, n_ae_dec2, n_ae_dec3, n_igae_enc1, \n",
    "                 n_igae_enc2, n_igae_dec1, n_igae_dec2, n_clusters, v = 1.0, device = None):\n",
    "        \n",
    "        super(DFCN, self).__init__()\n",
    "        self.autoencoder = AutoEncoder(n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3, n_ae_dec1, n_ae_dec2, n_ae_dec3)\n",
    "        self.gae = IGAE(n_input, n_z, n_igae_enc1, n_igae_enc2,n_igae_dec1, n_igae_dec2)\n",
    "\n",
    "        self.alpha = nn.Parameter(nn.init.constant_(torch.zeros(n_node, n_z), 0.5), requires_grad=True).to(device)\n",
    "\n",
    "        self.cluster_layer = nn.Parameter(torch.Tensor(n_clusters, n_z), requires_grad = True)\n",
    "        nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "        self.v = v\n",
    "        self.beta = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        z_ae = self.autoencoder.encoder(x)\n",
    "        z_igae, adj_igae = self.gae.encoder(x, adj)\n",
    "        z_i = self.alpha * z_ae + (1 - self.alpha) * z_igae\n",
    "        z_l = torch.spmm(adj, z_i)\n",
    "        s = F.softmax(torch.mm(z_l, z_l.t()), dim = 1)\n",
    "        z_g = torch.mm(s, z_l)\n",
    "        z_tilde = self.beta * z_g + z_l\n",
    "\n",
    "        xhat_ae = self.autoencoder.decoder(z_ae)\n",
    "        xhat_igae, adj_igae_dec = self.gae.decoder(z_igae, adj)\n",
    "\n",
    "        adj_hat = adj_igae + adj_igae_dec\n",
    "\n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow((z_tilde).unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q = q.pow((self.v + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "\n",
    "        q_ae = 1.0 / (1.0 + torch.sum(torch.pow(z_ae.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q_ae = q_ae.pow((self.v + 1.0) / 2.0)\n",
    "        q_ae = (q_ae.t() / torch.sum(q_ae, 1)).t()\n",
    "\n",
    "        q_igae = 1.0 / (1.0 + torch.sum(torch.pow(z_igae.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q_igae = q_igae.pow((self.v + 1.0) / 2.0)\n",
    "        q_igae = (q_igae.t() / torch.sum(q_igae, 1)).t()\n",
    "\n",
    "        return xhat_ae, xhat_igae, adj_hat, z_ae, z_igae, q, q_ae, q_igae, z_tilde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z = 20\n",
    "n_clusters = 4\n",
    "\n",
    "n_ae_enc1 = 128\n",
    "n_ae_enc2 = 256\n",
    "n_ae_enc3 = 120\n",
    "\n",
    "n_ae_dec1 = 120\n",
    "n_ae_dec2 = 256\n",
    "n_ae_dec3 = 128\n",
    "\n",
    "n_igae_enc1 = 128\n",
    "n_igae_enc2 = 256\n",
    "n_igae_dec1 = 256\n",
    "n_igae_dec2 = 128\n",
    "\n",
    "\n",
    "model = DFCN(X.size()[0], n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3, n_ae_dec1, n_ae_dec2, n_ae_dec3, \n",
    "             n_igae_enc1, n_igae_enc2, n_igae_dec1, n_igae_dec2, n_clusters, device = device).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eva, target_distribution\n",
    "\n",
    "acc_reuslt = []\n",
    "nmi_result = []\n",
    "ari_result = []\n",
    "f1_result = []\n",
    "\n",
    "def train(model, num_epoch, data, adj, label, lr, pre_model_save_path, final_model_save_path, \n",
    "          n_clusters, original_acc, gamma_value, lambda_value, device ):\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    model.load_state_dict(torch.load(pre_model_save_path, map_location='cpu'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        xhat_ae, xhat_igae, adj_hat, z_ae, z_igae, _, _, _, z_tilde = model(data, adj)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "    cluster_id = kmeans.fit_predict(z_tilde.data.cpu().numpy())\n",
    "    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "    eva(label, cluster_id, 'Initialization')\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        x_hat, z_hat, adj_hat, z_ae, z_igae, q, q1, q2, z_tilde = model(data, adj)\n",
    "\n",
    "        tmp_q = q.data\n",
    "        p = target_distribution(tmp_q)\n",
    "\n",
    "        loss_ae = F.mse_loss(x_hat, data)\n",
    "        loss_w = F.mse_loss(z_hat, torch.spmm(adj, data))\n",
    "        loss_a = F.mse_loss(adj_hat, adj.to_dense())\n",
    "        loss_igae = loss_w + gamma_value * loss_a\n",
    "        loss_kl = F.kl_div((q.log() + q1.log() + q2.log()) / 3, p, reduction='batchmean')\n",
    "        loss = loss_ae + loss_igae + lambda_value * loss_kl\n",
    "        print('{} loss: {}'.format(epoch, loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=20).fit(z_tilde.data.cpu().numpy())\n",
    "\n",
    "        acc, nmi, ari, f1 = eva(label, kmeans.labels_, epoch)\n",
    "        acc_reuslt.append(acc)\n",
    "        nmi_result.append(nmi)\n",
    "        ari_result.append(ari)\n",
    "        f1_result.append(f1)\n",
    "\n",
    "        if acc > original_acc:\n",
    "            original_acc = acc\n",
    "            torch.save(model.state_dict(), final_model_save_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_pretrain.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_Initialization :acc 0.4560 , nmi 0.1202 , ari 0.0996 , f1 0.4495\n",
      "0 loss: 0.29037588834762573\n",
      "Epoch_0 :acc 0.4575 , nmi 0.1221 , ari 0.1005 , f1 0.4513\n",
      "1 loss: 0.2821996808052063\n",
      "Epoch_1 :acc 0.4195 , nmi 0.0830 , ari 0.0794 , f1 0.3886\n",
      "2 loss: 0.2772257924079895\n",
      "Epoch_2 :acc 0.4109 , nmi 0.0733 , ari 0.0702 , f1 0.3848\n",
      "3 loss: 0.2744276821613312\n",
      "Epoch_3 :acc 0.3976 , nmi 0.0636 , ari 0.0609 , f1 0.3743\n",
      "4 loss: 0.272895485162735\n",
      "Epoch_4 :acc 0.3973 , nmi 0.0601 , ari 0.0582 , f1 0.3764\n",
      "5 loss: 0.2719300091266632\n",
      "Epoch_5 :acc 0.4010 , nmi 0.0593 , ari 0.0572 , f1 0.3846\n",
      "6 loss: 0.2711372673511505\n",
      "Epoch_6 :acc 0.4003 , nmi 0.0570 , ari 0.0557 , f1 0.3859\n",
      "7 loss: 0.27038708329200745\n",
      "Epoch_7 :acc 0.3458 , nmi 0.0421 , ari 0.0345 , f1 0.3332\n",
      "8 loss: 0.26969286799430847\n",
      "Epoch_8 :acc 0.3436 , nmi 0.0395 , ari 0.0317 , f1 0.3329\n",
      "9 loss: 0.2691015601158142\n",
      "Epoch_9 :acc 0.3416 , nmi 0.0390 , ari 0.0315 , f1 0.3308\n",
      "10 loss: 0.26861950755119324\n",
      "Epoch_10 :acc 0.3419 , nmi 0.0380 , ari 0.0306 , f1 0.3310\n",
      "11 loss: 0.2681967318058014\n",
      "Epoch_11 :acc 0.3414 , nmi 0.0376 , ari 0.0306 , f1 0.3300\n",
      "12 loss: 0.26778557896614075\n",
      "Epoch_12 :acc 0.3387 , nmi 0.0369 , ari 0.0299 , f1 0.3275\n",
      "13 loss: 0.26735469698905945\n",
      "Epoch_13 :acc 0.3369 , nmi 0.0372 , ari 0.0301 , f1 0.3255\n",
      "14 loss: 0.2669033706188202\n",
      "Epoch_14 :acc 0.3362 , nmi 0.0369 , ari 0.0300 , f1 0.3235\n",
      "15 loss: 0.26645946502685547\n",
      "Epoch_15 :acc 0.3352 , nmi 0.0369 , ari 0.0298 , f1 0.3228\n",
      "16 loss: 0.266069233417511\n",
      "Epoch_16 :acc 0.3345 , nmi 0.0369 , ari 0.0291 , f1 0.3222\n",
      "17 loss: 0.26578807830810547\n",
      "Epoch_17 :acc 0.3330 , nmi 0.0368 , ari 0.0291 , f1 0.3203\n",
      "18 loss: 0.2656644880771637\n",
      "Epoch_18 :acc 0.3325 , nmi 0.0369 , ari 0.0293 , f1 0.3188\n",
      "19 loss: 0.2657279670238495\n",
      "Epoch_19 :acc 0.3310 , nmi 0.0368 , ari 0.0289 , f1 0.3180\n",
      "20 loss: 0.2659735083580017\n",
      "Epoch_20 :acc 0.3315 , nmi 0.0371 , ari 0.0291 , f1 0.3184\n",
      "21 loss: 0.26637792587280273\n",
      "Epoch_21 :acc 0.3308 , nmi 0.0371 , ari 0.0289 , f1 0.3175\n",
      "22 loss: 0.2668989300727844\n",
      "Epoch_22 :acc 0.3286 , nmi 0.0367 , ari 0.0284 , f1 0.3156\n",
      "23 loss: 0.26750117540359497\n",
      "Epoch_23 :acc 0.3308 , nmi 0.0370 , ari 0.0289 , f1 0.3178\n",
      "24 loss: 0.26819270849227905\n",
      "Epoch_24 :acc 0.3278 , nmi 0.0372 , ari 0.0287 , f1 0.3134\n",
      "25 loss: 0.26905953884124756\n",
      "Epoch_25 :acc 0.3273 , nmi 0.0373 , ari 0.0288 , f1 0.3126\n",
      "26 loss: 0.270246684551239\n",
      "Epoch_26 :acc 0.3281 , nmi 0.0379 , ari 0.0292 , f1 0.3129\n",
      "27 loss: 0.27192020416259766\n",
      "Epoch_27 :acc 0.3281 , nmi 0.0380 , ari 0.0293 , f1 0.3131\n",
      "28 loss: 0.2740626633167267\n",
      "Epoch_28 :acc 0.3283 , nmi 0.0393 , ari 0.0298 , f1 0.3130\n",
      "29 loss: 0.27632224559783936\n",
      "Epoch_29 :acc 0.3266 , nmi 0.0398 , ari 0.0302 , f1 0.3109\n",
      "30 loss: 0.27831628918647766\n",
      "Epoch_30 :acc 0.3278 , nmi 0.0393 , ari 0.0300 , f1 0.3107\n",
      "31 loss: 0.27998965978622437\n",
      "Epoch_31 :acc 0.3271 , nmi 0.0404 , ari 0.0311 , f1 0.3094\n",
      "32 loss: 0.28152817487716675\n",
      "Epoch_32 :acc 0.3298 , nmi 0.0427 , ari 0.0329 , f1 0.3123\n",
      "33 loss: 0.2831358313560486\n",
      "Epoch_33 :acc 0.3318 , nmi 0.0436 , ari 0.0336 , f1 0.3124\n",
      "34 loss: 0.2849094867706299\n",
      "Epoch_34 :acc 0.3333 , nmi 0.0452 , ari 0.0348 , f1 0.3136\n",
      "35 loss: 0.28684377670288086\n",
      "Epoch_35 :acc 0.3325 , nmi 0.0454 , ari 0.0355 , f1 0.3117\n",
      "36 loss: 0.2889055013656616\n",
      "Epoch_36 :acc 0.3315 , nmi 0.0469 , ari 0.0362 , f1 0.3109\n",
      "37 loss: 0.2910820543766022\n",
      "Epoch_37 :acc 0.3330 , nmi 0.0476 , ari 0.0370 , f1 0.3121\n",
      "38 loss: 0.29339998960494995\n",
      "Epoch_38 :acc 0.3350 , nmi 0.0478 , ari 0.0371 , f1 0.3140\n",
      "39 loss: 0.29590529203414917\n",
      "Epoch_39 :acc 0.3352 , nmi 0.0479 , ari 0.0375 , f1 0.3139\n",
      "40 loss: 0.29863911867141724\n",
      "Epoch_40 :acc 0.3352 , nmi 0.0498 , ari 0.0391 , f1 0.3138\n",
      "41 loss: 0.301603764295578\n",
      "Epoch_41 :acc 0.3367 , nmi 0.0498 , ari 0.0393 , f1 0.3150\n",
      "42 loss: 0.30473965406417847\n",
      "Epoch_42 :acc 0.3404 , nmi 0.0503 , ari 0.0404 , f1 0.3179\n",
      "43 loss: 0.30794575810432434\n",
      "Epoch_43 :acc 0.3404 , nmi 0.0511 , ari 0.0407 , f1 0.3209\n",
      "44 loss: 0.31111738085746765\n",
      "Epoch_44 :acc 0.3421 , nmi 0.0519 , ari 0.0418 , f1 0.3224\n",
      "45 loss: 0.3141917884349823\n",
      "Epoch_45 :acc 0.3411 , nmi 0.0527 , ari 0.0425 , f1 0.3213\n",
      "46 loss: 0.3171755075454712\n",
      "Epoch_46 :acc 0.3439 , nmi 0.0542 , ari 0.0438 , f1 0.3244\n",
      "47 loss: 0.3201550543308258\n",
      "Epoch_47 :acc 0.3446 , nmi 0.0557 , ari 0.0454 , f1 0.3247\n",
      "48 loss: 0.32324546575546265\n",
      "Epoch_48 :acc 0.3466 , nmi 0.0573 , ari 0.0466 , f1 0.3269\n",
      "49 loss: 0.32652801275253296\n",
      "Epoch_49 :acc 0.3475 , nmi 0.0580 , ari 0.0465 , f1 0.3312\n",
      "50 loss: 0.33002474904060364\n",
      "Epoch_50 :acc 0.3490 , nmi 0.0584 , ari 0.0474 , f1 0.3385\n",
      "51 loss: 0.33373314142227173\n",
      "Epoch_51 :acc 0.3500 , nmi 0.0581 , ari 0.0474 , f1 0.3397\n",
      "52 loss: 0.33763155341148376\n",
      "Epoch_52 :acc 0.3508 , nmi 0.0591 , ari 0.0487 , f1 0.3346\n",
      "53 loss: 0.34163543581962585\n",
      "Epoch_53 :acc 0.3530 , nmi 0.0595 , ari 0.0491 , f1 0.3372\n",
      "54 loss: 0.3456175625324249\n",
      "Epoch_54 :acc 0.3569 , nmi 0.0610 , ari 0.0506 , f1 0.3389\n",
      "55 loss: 0.3494938313961029\n",
      "Epoch_55 :acc 0.3584 , nmi 0.0615 , ari 0.0515 , f1 0.3411\n",
      "56 loss: 0.35329145193099976\n",
      "Epoch_56 :acc 0.3586 , nmi 0.0620 , ari 0.0520 , f1 0.3413\n",
      "57 loss: 0.3571123480796814\n",
      "Epoch_57 :acc 0.3614 , nmi 0.0630 , ari 0.0533 , f1 0.3440\n",
      "58 loss: 0.3610549569129944\n",
      "Epoch_58 :acc 0.3609 , nmi 0.0634 , ari 0.0536 , f1 0.3435\n",
      "59 loss: 0.36516761779785156\n",
      "Epoch_59 :acc 0.3623 , nmi 0.0645 , ari 0.0543 , f1 0.3454\n",
      "60 loss: 0.36943817138671875\n",
      "Epoch_60 :acc 0.3648 , nmi 0.0655 , ari 0.0552 , f1 0.3475\n",
      "61 loss: 0.3738207221031189\n",
      "Epoch_61 :acc 0.3658 , nmi 0.0657 , ari 0.0554 , f1 0.3485\n",
      "62 loss: 0.37825989723205566\n",
      "Epoch_62 :acc 0.3680 , nmi 0.0672 , ari 0.0571 , f1 0.3506\n",
      "63 loss: 0.3827347755432129\n",
      "Epoch_63 :acc 0.3685 , nmi 0.0681 , ari 0.0584 , f1 0.3507\n",
      "64 loss: 0.38727879524230957\n",
      "Epoch_64 :acc 0.3707 , nmi 0.0697 , ari 0.0598 , f1 0.3528\n",
      "65 loss: 0.39193981885910034\n",
      "Epoch_65 :acc 0.3717 , nmi 0.0706 , ari 0.0607 , f1 0.3533\n",
      "66 loss: 0.3967379331588745\n",
      "Epoch_66 :acc 0.3752 , nmi 0.0720 , ari 0.0609 , f1 0.3582\n",
      "67 loss: 0.4017062187194824\n",
      "Epoch_67 :acc 0.3756 , nmi 0.0729 , ari 0.0617 , f1 0.3583\n",
      "68 loss: 0.4068959355354309\n",
      "Epoch_68 :acc 0.3791 , nmi 0.0751 , ari 0.0627 , f1 0.3634\n",
      "69 loss: 0.41227585077285767\n",
      "Epoch_69 :acc 0.3803 , nmi 0.0768 , ari 0.0660 , f1 0.3610\n",
      "70 loss: 0.417743980884552\n",
      "Epoch_70 :acc 0.3848 , nmi 0.0784 , ari 0.0676 , f1 0.3654\n",
      "71 loss: 0.4232718348503113\n",
      "Epoch_71 :acc 0.3867 , nmi 0.0797 , ari 0.0685 , f1 0.3668\n",
      "72 loss: 0.4289359748363495\n",
      "Epoch_72 :acc 0.3875 , nmi 0.0848 , ari 0.0728 , f1 0.3631\n",
      "73 loss: 0.4347812533378601\n",
      "Epoch_73 :acc 0.3899 , nmi 0.0858 , ari 0.0736 , f1 0.3659\n",
      "74 loss: 0.44074177742004395\n",
      "Epoch_74 :acc 0.3907 , nmi 0.0844 , ari 0.0725 , f1 0.3681\n",
      "75 loss: 0.44677668809890747\n",
      "Epoch_75 :acc 0.3927 , nmi 0.0853 , ari 0.0731 , f1 0.3698\n",
      "76 loss: 0.45295608043670654\n",
      "Epoch_76 :acc 0.3934 , nmi 0.0850 , ari 0.0731 , f1 0.3702\n",
      "77 loss: 0.4591342508792877\n",
      "Epoch_77 :acc 0.3951 , nmi 0.0850 , ari 0.0736 , f1 0.3718\n",
      "78 loss: 0.46523207426071167\n",
      "Epoch_78 :acc 0.3973 , nmi 0.0848 , ari 0.0739 , f1 0.3738\n",
      "79 loss: 0.47139525413513184\n",
      "Epoch_79 :acc 0.3986 , nmi 0.0843 , ari 0.0727 , f1 0.3784\n",
      "80 loss: 0.4775087535381317\n",
      "Epoch_80 :acc 0.4018 , nmi 0.0852 , ari 0.0741 , f1 0.3813\n",
      "81 loss: 0.4836128354072571\n",
      "Epoch_81 :acc 0.4008 , nmi 0.0841 , ari 0.0734 , f1 0.3839\n",
      "82 loss: 0.4898596704006195\n",
      "Epoch_82 :acc 0.4033 , nmi 0.0839 , ari 0.0731 , f1 0.3870\n",
      "83 loss: 0.4960194230079651\n",
      "Epoch_83 :acc 0.4040 , nmi 0.0842 , ari 0.0737 , f1 0.3877\n",
      "84 loss: 0.5023024678230286\n",
      "Epoch_84 :acc 0.4045 , nmi 0.0841 , ari 0.0716 , f1 0.3904\n",
      "85 loss: 0.5085130929946899\n",
      "Epoch_85 :acc 0.4062 , nmi 0.0841 , ari 0.0728 , f1 0.3910\n",
      "86 loss: 0.5148910284042358\n",
      "Epoch_86 :acc 0.4045 , nmi 0.0846 , ari 0.0722 , f1 0.3892\n",
      "87 loss: 0.5211233496665955\n",
      "Epoch_87 :acc 0.4065 , nmi 0.0840 , ari 0.0734 , f1 0.3904\n",
      "88 loss: 0.5279178619384766\n",
      "Epoch_88 :acc 0.4062 , nmi 0.0843 , ari 0.0734 , f1 0.3898\n",
      "89 loss: 0.5335924029350281\n",
      "Epoch_89 :acc 0.4062 , nmi 0.0859 , ari 0.0745 , f1 0.3882\n",
      "90 loss: 0.5420079231262207\n",
      "Epoch_90 :acc 0.4065 , nmi 0.0858 , ari 0.0746 , f1 0.3883\n",
      "91 loss: 0.5466771125793457\n",
      "Epoch_91 :acc 0.4065 , nmi 0.0861 , ari 0.0748 , f1 0.3878\n",
      "92 loss: 0.5534758567810059\n",
      "Epoch_92 :acc 0.4065 , nmi 0.0867 , ari 0.0745 , f1 0.3882\n",
      "93 loss: 0.561356782913208\n",
      "Epoch_93 :acc 0.4062 , nmi 0.0862 , ari 0.0753 , f1 0.3873\n",
      "94 loss: 0.5667991638183594\n",
      "Epoch_94 :acc 0.4028 , nmi 0.0837 , ari 0.0723 , f1 0.3858\n",
      "95 loss: 0.5733413696289062\n",
      "Epoch_95 :acc 0.4047 , nmi 0.0842 , ari 0.0729 , f1 0.3871\n",
      "96 loss: 0.5813101530075073\n",
      "Epoch_96 :acc 0.4052 , nmi 0.0846 , ari 0.0735 , f1 0.3871\n",
      "97 loss: 0.5874103307723999\n",
      "Epoch_97 :acc 0.4065 , nmi 0.0851 , ari 0.0746 , f1 0.3873\n",
      "98 loss: 0.5936659574508667\n",
      "Epoch_98 :acc 0.4045 , nmi 0.0818 , ari 0.0713 , f1 0.3867\n",
      "99 loss: 0.6017289757728577\n",
      "Epoch_99 :acc 0.4057 , nmi 0.0834 , ari 0.0741 , f1 0.3861\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "lr = 5e-4\n",
    "pre_model_save_path = 'model_pretrain.pkl'\n",
    "final_model_save_path = 'model_final.pkl'\n",
    "\n",
    "\n",
    "\n",
    "train(model, num_epoch,  X, adj, label, lr,  pre_model_save_path, final_model_save_path,\n",
    "      n_clusters, -1, 0.1, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
