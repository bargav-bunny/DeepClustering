{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from scipy import sparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('Data/dblp.txt')\n",
    "y = np.loadtxt('Data/dblp_label.txt', dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4057, 334)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 0 ... 3 3 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 50\n",
    "\n",
    "pca = PCA(n_components=n_input)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X = torch.Tensor(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 1833],\n",
       "       [   2,   97],\n",
       "       [   2, 1561],\n",
       "       ...,\n",
       "       [4055, 1286],\n",
       "       [4055, 2202],\n",
       "       [4055, 3007]], dtype=int32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = np.loadtxt('Data/dblp_graph.txt', dtype=np.int32)\n",
    "\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adj = sparse.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(n, n), dtype=np.float32)\n",
    "\n",
    "adj = adj + sparse.eye(adj.shape[0])\n",
    "\n",
    "degrees = np.array(adj.sum(axis=1)).flatten()\n",
    "\n",
    "D_inv_sqrt = sparse.diags(np.power(degrees, -0.5))\n",
    "\n",
    "adj = D_inv_sqrt.dot(adj).dot(D_inv_sqrt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_torch_sparse_tensor(sparse_mx):\n",
    "\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "\n",
    "    values = torch.tensor(sparse_mx.data, dtype=torch.float32)\n",
    "\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = to_torch_sparse_tensor(adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load available Device \n",
    "\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mps_device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = mps_device\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    1,    1,  ..., 4055, 4055, 4056],\n",
       "                       [   0,    1, 1833,  ..., 3007, 4055, 4056]]),\n",
       "       values=tensor([1.0000, 0.5000, 0.4082,  ..., 0.2582, 0.2000, 1.0000]),\n",
       "       size=(4057, 4057), nnz=11113, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj  = adj.to(device)\n",
    "X = X.to(device)\n",
    "\n",
    "label = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, hidden1_dim, hidden2_dim, hidden3_dim):\n",
    "        super(AE_Encoder, self).__init__()\n",
    "\n",
    "        self.enc1 = nn.Linear(in_channels, hidden1_dim)\n",
    "        self.enc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.enc3 = nn.Linear(hidden2_dim, hidden3_dim)\n",
    "        self.z_layer = nn.Linear(hidden3_dim, out_channels)\n",
    "        self.act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.act_fn(self.enc1(x))\n",
    "        z = self.act_fn(self.enc2(z))\n",
    "        z = self.act_fn(self.enc3(z))\n",
    "\n",
    "        z = self.z_layer(z)\n",
    "        \n",
    "        return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AE_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, hidden1_dim, hidden2_dim, hidden3_dim):\n",
    "        super(AE_Decoder, self).__init__()\n",
    "\n",
    "        self.dec1 = nn.Linear(in_channels, hidden1_dim)\n",
    "        self.dec2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.dec3 = nn.Linear(hidden2_dim, hidden3_dim)\n",
    "        self.xhat_layer = nn.Linear(hidden3_dim, out_channels)\n",
    "        self.act_fn = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        xhat = self.act_fn(self.dec1(z))\n",
    "        xhat = self.act_fn(self.dec2(xhat))\n",
    "        xhat = self.act_fn(self.dec3(xhat))\n",
    "\n",
    "        xhat = self.xhat_layer(xhat)\n",
    "        \n",
    "        return xhat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3, n_ae_dec1, n_ae_dec2, n_ae_dec3):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = AE_Encoder(n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3)\n",
    "        self.decoder = AE_Decoder(n_z, n_input, n_ae_dec1, n_ae_dec2, n_ae_dec3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        xhat = self.decoder(z)\n",
    "\n",
    "        return xhat, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, active = False):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.active = active\n",
    "        self.fc = nn.Linear(in_channels, out_channels, )\n",
    "        self.act_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        if self.active:\n",
    "            support = self.act_fn(self.fc(x))\n",
    "        else:\n",
    "            support = self.fc(x)\n",
    "\n",
    "        return torch.spmm(adj, support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGAE_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, hidden1_dim, hidden2_dim):\n",
    "        super(IGAE_Encoder, self).__init__()\n",
    "\n",
    "        self.enc1 = GNNLayer(in_channels, hidden1_dim, active=True)\n",
    "        self.enc2 = GNNLayer(hidden1_dim, hidden2_dim, active=True)\n",
    "        self.enc3 = GNNLayer(hidden2_dim, out_channels, active=False)\n",
    "\n",
    "        self.sigmoid_fn = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        z = self.enc1(x, adj)\n",
    "        z = self.enc2(z, adj)\n",
    "        z = self.enc3(z, adj)\n",
    "\n",
    "        adj_hat = self.sigmoid_fn(torch.mm(z, z.t()))\n",
    "\n",
    "        return z, adj_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGAE_Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, hidden1_dim, hidden2_dim):\n",
    "        super(IGAE_Decoder, self).__init__()\n",
    "\n",
    "        self.dec1 = GNNLayer(in_channels, hidden1_dim, active = True)\n",
    "        self.dec2 = GNNLayer(hidden1_dim, hidden2_dim, active = True)\n",
    "        self.dec3 = GNNLayer(hidden2_dim, out_channels, active=True)\n",
    "\n",
    "        self.sigmoid_fn = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z, adj):\n",
    "        xhat = self.dec1(z, adj)\n",
    "        xhat = self.dec2(xhat, adj)\n",
    "        xhat = self.dec3(xhat, adj)\n",
    "\n",
    "        adj_hat = self.sigmoid_fn(torch.mm(xhat, xhat.t()))\n",
    "\n",
    "        return xhat, adj_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGAE(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_z, n_igae_enc1, n_igae_enc2, n_igae_dec1, n_igae_dec2):\n",
    "        super(IGAE, self).__init__()\n",
    "\n",
    "        self.encoder = IGAE_Encoder(n_input, n_z, n_igae_enc1, n_igae_enc2)\n",
    "        self.decoder = IGAE_Decoder(n_z, n_input, n_igae_dec1, n_igae_dec2)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        z, adj_enc = self.encoder(x, adj)\n",
    "        xhat, adj_dec = self.decoder(z, adj)\n",
    "\n",
    "        adj_hat = adj_enc + adj_dec\n",
    "\n",
    "        return z, xhat, adj_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_node, n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3,n_ae_dec1, n_ae_dec2, n_ae_dec3, n_igae_enc1, \n",
    "                 n_igae_enc2, n_igae_dec1, n_igae_dec2, n_clusters, v = 1.0, device = None):\n",
    "        \n",
    "        super(DFCN, self).__init__()\n",
    "        self.autoencoder = AutoEncoder(n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3, n_ae_dec1, n_ae_dec2, n_ae_dec3)\n",
    "        self.gae = IGAE(n_input, n_z, n_igae_enc1, n_igae_enc2,n_igae_dec1, n_igae_dec2)\n",
    "\n",
    "        self.alpha = nn.Parameter(nn.init.constant_(torch.zeros(n_node, n_z), 0.5), requires_grad=True).to(device)\n",
    "\n",
    "        self.cluster_layer = nn.Parameter(torch.Tensor(n_clusters, n_z), requires_grad = True)\n",
    "        nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "        self.v = v\n",
    "        self.beta = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        z_ae = self.autoencoder.encoder(x)\n",
    "        z_igae, adj_igae = self.gae.encoder(x, adj)\n",
    "        z_i = self.alpha * z_ae + (1 - self.alpha) * z_igae\n",
    "        z_l = torch.spmm(adj, z_i)\n",
    "        s = F.softmax(torch.mm(z_l, z_l.t()), dim = 1)\n",
    "        z_g = torch.mm(s, z_l)\n",
    "        z_tilde = self.beta * z_g + z_l\n",
    "\n",
    "        xhat_ae = self.autoencoder.decoder(z_ae)\n",
    "        xhat_igae, adj_igae_dec = self.gae.decoder(z_igae, adj)\n",
    "\n",
    "        adj_hat = adj_igae + adj_igae_dec\n",
    "\n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow((z_tilde).unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q = q.pow((self.v + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "\n",
    "        q_ae = 1.0 / (1.0 + torch.sum(torch.pow(z_ae.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q_ae = q_ae.pow((self.v + 1.0) / 2.0)\n",
    "        q_ae = (q_ae.t() / torch.sum(q_ae, 1)).t()\n",
    "\n",
    "        q_igae = 1.0 / (1.0 + torch.sum(torch.pow(z_igae.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q_igae = q_igae.pow((self.v + 1.0) / 2.0)\n",
    "        q_igae = (q_igae.t() / torch.sum(q_igae, 1)).t()\n",
    "\n",
    "        return xhat_ae, xhat_igae, adj_hat, z_ae, z_igae, q, q_ae, q_igae, z_tilde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_z = 20\n",
    "n_clusters = 4\n",
    "\n",
    "n_ae_enc1 = 128\n",
    "n_ae_enc2 = 256\n",
    "n_ae_enc3 = 120\n",
    "\n",
    "n_ae_dec1 = 120\n",
    "n_ae_dec2 = 256\n",
    "n_ae_dec3 = 128\n",
    "\n",
    "n_igae_enc1 = 128\n",
    "n_igae_enc2 = 256\n",
    "n_igae_dec1 = 256\n",
    "n_igae_dec2 = 128\n",
    "\n",
    "\n",
    "model = DFCN(X.size()[0], n_input, n_z, n_ae_enc1, n_ae_enc2, n_ae_enc3, n_ae_dec1, n_ae_dec2, n_ae_dec3, \n",
    "             n_igae_enc1, n_igae_enc2, n_igae_dec1, n_igae_dec2, n_clusters, device = device).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import eva, target_distribution\n",
    "\n",
    "acc_reuslt = []\n",
    "nmi_result = []\n",
    "ari_result = []\n",
    "f1_result = []\n",
    "\n",
    "def train(model, num_epoch, data, adj, label, lr, pre_model_save_path, final_model_save_path, \n",
    "          n_clusters, original_acc, gamma_value, lambda_value, device ):\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    model.load_state_dict(torch.load(pre_model_save_path, map_location='cpu'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        xhat_ae, xhat_igae, adj_hat, z_ae, z_igae, _, _, _, z_tilde = model(data, adj)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
    "    cluster_id = kmeans.fit_predict(z_tilde.data.cpu().numpy())\n",
    "    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "    eva(label, cluster_id, 'Initialization')\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        x_hat, z_hat, adj_hat, z_ae, z_igae, q, q1, q2, z_tilde = model(data, adj)\n",
    "\n",
    "        tmp_q = q.data\n",
    "        p = target_distribution(tmp_q)\n",
    "\n",
    "        loss_ae = F.mse_loss(x_hat, data)\n",
    "        loss_w = F.mse_loss(z_hat, torch.spmm(adj, data))\n",
    "        loss_a = F.mse_loss(adj_hat, adj.to_dense())\n",
    "        loss_igae = loss_w + gamma_value * loss_a\n",
    "        loss_kl = F.kl_div((q.log() + q1.log() + q2.log()) / 3, p, reduction='batchmean')\n",
    "        loss = loss_ae + loss_igae + lambda_value * loss_kl\n",
    "        print('{} loss: {}'.format(epoch, loss))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=20).fit(z_tilde.data.cpu().numpy())\n",
    "\n",
    "        acc, nmi, ari, f1 = eva(label, kmeans.labels_, epoch)\n",
    "        acc_reuslt.append(acc)\n",
    "        nmi_result.append(nmi)\n",
    "        ari_result.append(ari)\n",
    "        f1_result.append(f1)\n",
    "\n",
    "        if acc > original_acc:\n",
    "            original_acc = acc\n",
    "            torch.save(model.state_dict(), final_model_save_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_pretrain.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch_Initialization :acc 0.5078 , nmi 0.1642 , ari 0.1562 , f1 0.5005\n",
      "0 loss: 0.2902512848377228\n",
      "Epoch_0 :acc 0.5053 , nmi 0.1620 , ari 0.1535 , f1 0.4982\n",
      "1 loss: 0.28229841589927673\n",
      "Epoch_1 :acc 0.4996 , nmi 0.1537 , ari 0.1467 , f1 0.4904\n",
      "2 loss: 0.2772531807422638\n",
      "Epoch_2 :acc 0.4821 , nmi 0.1430 , ari 0.1304 , f1 0.4710\n",
      "3 loss: 0.27427980303764343\n",
      "Epoch_3 :acc 0.4673 , nmi 0.1377 , ari 0.1215 , f1 0.4569\n",
      "4 loss: 0.27259185910224915\n",
      "Epoch_4 :acc 0.4550 , nmi 0.1347 , ari 0.1151 , f1 0.4455\n",
      "5 loss: 0.271635502576828\n",
      "Epoch_5 :acc 0.4501 , nmi 0.1324 , ari 0.1116 , f1 0.4393\n",
      "6 loss: 0.2710537314414978\n",
      "Epoch_6 :acc 0.4493 , nmi 0.1332 , ari 0.1111 , f1 0.4386\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m pre_model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_pretrain.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m final_model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_final.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mpre_model_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_model_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[103], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epoch, data, adj, label, lr, pre_model_save_path, final_model_save_path, n_clusters, original_acc, gamma_value, lambda_value, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m eva(label, cluster_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialization\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epoch):\n\u001b[0;32m---> 25\u001b[0m     x_hat, z_hat, adj_hat, z_ae, z_igae, q, q1, q2, z_tilde \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     tmp_q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     28\u001b[0m     p \u001b[38;5;241m=\u001b[39m target_distribution(tmp_q)\n",
      "File \u001b[0;32m~/anaconda3/envs/clean_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[101], line 29\u001b[0m, in \u001b[0;36mDFCN.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     26\u001b[0m z_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m*\u001b[39m z_g \u001b[38;5;241m+\u001b[39m z_l\n\u001b[1;32m     28\u001b[0m xhat_ae \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mdecoder(z_ae)\n\u001b[0;32m---> 29\u001b[0m xhat_igae, adj_igae_dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_igae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m adj_hat \u001b[38;5;241m=\u001b[39m adj_igae \u001b[38;5;241m+\u001b[39m adj_igae_dec\n\u001b[1;32m     33\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mpow((z_tilde)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_layer, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv)\n",
      "File \u001b[0;32m~/anaconda3/envs/clean_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[99], line 17\u001b[0m, in \u001b[0;36mIGAE_Decoder.forward\u001b[0;34m(self, z, adj)\u001b[0m\n\u001b[1;32m     14\u001b[0m xhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec2(xhat, adj)\n\u001b[1;32m     15\u001b[0m xhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec3(xhat, adj)\n\u001b[0;32m---> 17\u001b[0m adj_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid_fn(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxhat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxhat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xhat, adj_hat\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "lr = 5e-4\n",
    "pre_model_save_path = 'model_pretrain.pkl'\n",
    "final_model_save_path = 'model_final.pkl'\n",
    "\n",
    "\n",
    "\n",
    "train(model, num_epoch,  X, adj, label, lr,  pre_model_save_path, final_model_save_path,\n",
    "      n_clusters, -1, 0.1, 10, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
